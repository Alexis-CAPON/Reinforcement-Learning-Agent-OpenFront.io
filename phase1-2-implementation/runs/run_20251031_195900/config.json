{
  "project_name": "OpenFrontIO-RL-Phase3",
  "version": "3.0.0-maskable-ppo",
  "game": {
    "map_name": "plains",
    "map_size": [
      50,
      50
    ],
    "num_players": 8,
    "opponent_difficulty": "easy",
    "max_ticks": 60000,
    "tick_interval_ms": 100
  },
  "environment": {
    "max_steps": 50000,
    "observation_space_type": "Dict",
    "observation_space": {
      "features": {
        "type": "Box",
        "shape": [
          12
        ],
        "description": "Base[5] + Strategic[5] + Temporal[2]: [tiles_owned, troops, gold, enemy_tiles, tick, troops_per_tile, avg_enemy_troops_per_tile, border_tile_ratio, rank_by_tiles, rank_by_troops, tiles_change_rate, troops_change_rate]"
      },
      "action_mask": {
        "type": "Box",
        "shape": [
          45
        ],
        "description": "Valid actions: 45 discrete actions (9 targets \u00d7 5 percentages). MaskablePPO enforces this mask!"
      },
      "neighbor_info": {
        "type": "Box",
        "shape": [
          8,
          2
        ],
        "description": "Per-neighbor info (8 neighbors x 2 features): [troops_normalized, tiles_normalized]. Aligned with action space (neighbor_info[i] \u2192 actions 5*(i+1) to 5*(i+1)+4)"
      },
      "player_info": {
        "type": "Box",
        "shape": [
          8,
          3
        ],
        "description": "Per-player info matrix (8 players x 3 features): [tiles_normalized, troops_normalized, is_neighbor]. First row is RL agent, rest are alive AI players."
      }
    },
    "action_space_type": "Discrete",
    "action_space": {
      "n": 45,
      "description": "Pure discrete action space for MaskablePPO. Each action represents a target-percentage combination.",
      "decoding": {
        "attack_target": "action // 5  (0=IDLE, 1-8=neighbors)",
        "attack_percentage": "action % 5  (0=0%, 1=25%, 2=50%, 3=75%, 4=100%)"
      },
      "percentage_values": [
        0.0,
        0.25,
        0.5,
        0.75,
        1.0
      ],
      "examples": {
        "0-4": "IDLE with all percentages (always valid)",
        "5-9": "Neighbor 1 with [0%, 25%, 50%, 75%, 100%]",
        "10-14": "Neighbor 2 with [0%, 25%, 50%, 75%, 100%]",
        "40-44": "Neighbor 8 with [0%, 25%, 50%, 75%, 100%]"
      }
    }
  },
  "reward": {
    "per_tile_change": 50,
    "per_step": -0.05,
    "action_bonus": 0.5,
    "enemy_kill_bonus": 8000,
    "military_strength_bonus": 1.0,
    "neighbor_strength_bonus": 5.0,
    "win_bonus": 15000,
    "loss_penalty": -10000,
    "timeout_penalty": -7500,
    "_comment": "Phase 3 (MaskablePPO): Same rewards as Phase 2, but with proper action masking and discretized attack percentages. Agent should learn more efficiently without wasting capacity on invalid actions."
  },
  "training": {
    "algorithm": "MaskablePPO",
    "total_timesteps": 2000000,
    "n_steps": 50000,
    "batch_size": 5000,
    "n_epochs": 10,
    "learning_rate": 0.0003,
    "learning_rate_schedule": "constant",
    "gamma": 0.99,
    "gae_lambda": 0.95,
    "clip_range": 0.2,
    "ent_coef": 0.05,
    "vf_coef": 0.5,
    "max_grad_norm": 0.5,
    "normalize_observations": false,
    "_comment": "Phase 3: Using MaskablePPO with pure discrete action space (45 actions = 9 targets \u00d7 5 percentages). Proper action masking enforced by algorithm. Increased total_timesteps to 2M since agent should learn more efficiently."
  },
  "logging": {
    "tensorboard_log": "./logs/tensorboard/",
    "checkpoint_freq": 100000,
    "eval_freq": 50000,
    "eval_episodes": 5
  }
}