{
  "project_name": "OpenFrontIO-RL-Phase1",
  "version": "1.0.1-dense-rewards",
  "game": {
    "map_name": "plains",
    "map_size": [
      50,
      50
    ],
    "num_players": 2,
    "opponent_difficulty": "easy",
    "max_ticks": 500,
    "tick_interval_ms": 100
  },
  "environment": {
    "max_steps": 1500,
    "observation_dim": 5,
    "action_dim": 9,
    "observation_features": [
      "tiles_owned",
      "troops",
      "gold",
      "enemy_tiles",
      "game_tick"
    ],
    "actions": [
      "IDLE",
      "ATTACK_NEIGHBOR_0",
      "ATTACK_NEIGHBOR_1",
      "ATTACK_NEIGHBOR_2",
      "ATTACK_NEIGHBOR_3",
      "ATTACK_NEIGHBOR_4",
      "ATTACK_NEIGHBOR_5",
      "ATTACK_NEIGHBOR_6",
      "ATTACK_NEIGHBOR_7"
    ]
  },
  "reward": {
    "per_tile_gained": 50,
    "per_tile_lost": -50,
    "per_step": -0.1,
    "win_bonus": 1000,
    "loss_penalty": -1000,
    "timeout_penalty": -500,
    "_comment": "Note: Dense rewards (territorial control, enemy penalty, troop growth) are implemented in code, not config"
  },
  "training": {
    "algorithm": "PPO",
    "total_timesteps": 200000,
    "n_steps": 2048,
    "batch_size": 256,
    "n_epochs": 10,
    "learning_rate": 0.0001,
    "gamma": 0.99,
    "gae_lambda": 0.95,
    "clip_range": 0.2,
    "ent_coef": 0.01,
    "vf_coef": 0.5,
    "max_grad_norm": 0.5
  },
  "logging": {
    "tensorboard_log": "./logs/tensorboard/",
    "checkpoint_freq": 10000,
    "eval_freq": 5000,
    "eval_episodes": 10
  }
}