# Phase 5: All Training Options

Phase 5 provides multiple training approaches for different map sizes and strategic requirements. This guide helps you choose the right one.

## Quick Decision Tree

```
Start here:

1. What's your map size?
   ‚îî‚îÄ ‚â§512√ó512 ‚Üí Single-scale (train.py)
   ‚îî‚îÄ ‚â•1024√ó1024 ‚Üí Multi-scale (train_multiscale.py)

2. Do you have disconnected territories often?
   ‚îî‚îÄ No ‚Üí Use standard training
   ‚îî‚îÄ Yes ‚Üí Use cluster-aware (train_clusters.py)

3. Do you want attention mechanisms?
   ‚îî‚îÄ No ‚Üí Use default
   ‚îî‚îÄ Yes ‚Üí model_multiscale_attention.py
```

## Option 1: Single-Scale Training (Standard)

**File**: `train.py`
**Environment**: `environment.py`
**Model**: Default CNN

### When to Use
- ‚úÖ Map size ‚â§512√ó512
- ‚úÖ Simple expansion strategy
- ‚úÖ Fast training needed
- ‚úÖ Compatible with Phase 3 models

### Observation
```python
{
    'map': (128, 128, 20),    # Downsampled spatial
    'global': (64,)            # Global features
}
```

### Action Space
```python
Discrete(45)  # 9 directions √ó 5 intensities
```

### Training Command
```bash
python train.py \
  --map plains \
  --num-bots 10 \
  --total-timesteps 1000000
```

### Performance
- **Training speed**: Fast (~5 hours for 1M steps on MPS)
- **Memory**: Low (~10 MB for 8 envs)
- **Strategic depth**: Moderate
- **Best for**: Small/medium maps

---

## Option 2: Multi-Scale Training (Large Maps)

**File**: `train_multiscale.py`
**Environment**: `environment_multiscale.py`
**Model**: `model_multiscale_attention.py` (with attention)

### When to Use
- ‚úÖ Map size ‚â•1024√ó1024
- ‚úÖ Need tactical precision
- ‚úÖ Complex terrain
- ‚úÖ Have GPU memory

### Observation
```python
{
    'global_map': (128, 128, 20),   # Strategic overview
    'local_map': (128, 128, 20),    # Tactical awareness
    'tactical_map': (64, 64, 20),   # Precise control
    'global': (64,)                  # Global features
}
```

### Action Space
```python
Discrete(45)  # Same as single-scale
```

### Architecture
```
Global CNN (128√ó128) ‚îÄ‚îÄ‚îê
Local CNN (128√ó128)  ‚îÄ‚îÄ‚î§‚îÄ‚Üí Cross-Attention ‚îÄ‚Üí Fusion ‚îÄ‚Üí Policy
Tactical CNN (64√ó64) ‚îÄ‚îÄ‚îò        ‚Üë
Global Features ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Training Command
```bash
python train_multiscale.py \
  --map australia_1024x1024 \
  --num-bots 10 \
  --local-view-size 256 \
  --total-timesteps 1000000
```

### Performance
- **Training speed**: Slower (~10-12 hours for 1M steps on MPS)
- **Memory**: Higher (~25 MB for 8 envs)
- **Strategic depth**: High (strategic + tactical)
- **Best for**: Large maps, complex terrain

---

## Option 3: Cluster-Aware Training (Split Territories)

**File**: `train_clusters.py`
**Environment**: `environment_clusters.py`
**Model**: Default CNN + action masking

### When to Use
- ‚úÖ Territory splits are common
- ‚úÖ Need multi-front warfare
- ‚úÖ Strategic territory control
- ‚úÖ Complex maps with choke points

### Observation
```python
{
    'map': (128, 128, 20),
    'global': (64,),
    'clusters': (5, 6)  # Up to 5 clusters with 6 features each
}
```

### Action Space
```python
MultiDiscrete([5, 9, 5])  # cluster_id, direction, intensity
# Total: 225 actions (with masking)
```

### Training Command
```bash
pip install sb3-contrib  # Required for MaskablePPO

python train_clusters.py \
  --map plains \
  --num-bots 10 \
  --total-timesteps 1000000
```

### Performance
- **Training speed**: Moderate (~6-7 hours for 1M steps on MPS)
- **Memory**: Low-medium (~12 MB for 8 envs)
- **Strategic depth**: Very high (cluster management)
- **Best for**: Maps with splits, multi-front warfare

---

## Combination Options

### Option 4: Multi-Scale + Clusters

Combine large map support with cluster awareness.

**Status**: Not yet implemented (TODO)

**Would provide**:
- Large map observation (3 scales)
- Cluster-aware actions
- Best of both worlds

**Implementation needed**:
- Extend `environment_multiscale.py` with cluster features
- Update observation space
- Add action masking

### Option 5: Single-Scale + Clusters

Already available via `train_clusters.py`!

Use for medium maps (512√ó512) with territory splits.

---

## Feature Comparison Matrix

| Feature | Single-Scale | Multi-Scale | Clusters | Multi+Clusters |
|---------|--------------|-------------|----------|----------------|
| **Map size** | ‚â§512 | ‚â•1024 | Any | ‚â•1024 |
| **Action space** | 45 | 45 | 225 | 225 |
| **Observation size** | Small | Large | Medium | Very large |
| **Training speed** | Fast | Slow | Medium | Slowest |
| **Memory usage** | Low | High | Low | Highest |
| **Split territories** | ‚ùå Poor | ‚ùå Poor | ‚úÖ Excellent | ‚úÖ Excellent |
| **Large maps** | ‚ùå Poor | ‚úÖ Excellent | ‚ö†Ô∏è OK | ‚úÖ Excellent |
| **Strategic depth** | ‚ö†Ô∏è OK | ‚úÖ Good | ‚úÖ Good | ‚úÖ Excellent |
| **Action masking** | ‚ùå No | ‚ùå No | ‚úÖ Yes | ‚úÖ Yes |
| **Attention** | ‚ùå No | ‚úÖ Yes | ‚ùå No | ‚úÖ Yes |

---

## Hardware Recommendations

### CUDA GPU (16GB+)
```bash
# Best option: Multi-scale with attention
python train_multiscale.py \
  --device cuda \
  --n-envs 8 \
  --batch-size 256 \
  --map your_1024x1024_map
```

### Apple Silicon (MPS)
```bash
# Good option: Clusters or single-scale
python train_clusters.py \
  --device mps \
  --n-envs 4 \
  --batch-size 128
```

### CPU Only
```bash
# Best option: Single-scale
python train.py \
  --device cpu \
  --n-envs 2 \
  --batch-size 64 \
  --map small_map
```

---

## Training Time Estimates (1M steps)

| Setup | Hardware | Time |
|-------|----------|------|
| Single-scale | MPS | ~5 hours |
| Single-scale | CUDA | ~2-3 hours |
| Multi-scale | MPS | ~10-12 hours |
| Multi-scale | CUDA | ~5-6 hours |
| Clusters | MPS | ~6-7 hours |
| Clusters | CUDA | ~3-4 hours |
| Multi+Clusters | MPS | ~15-18 hours |
| Multi+Clusters | CUDA | ~8-10 hours |

---

## Recommended Workflow

### For Small Maps (256√ó256, australia_100x100)

```bash
# Phase 1: Quick test (100K steps, ~30 min)
python train.py --map australia_100x100 --total-timesteps 100000

# Phase 2: Full training (1M steps, ~5 hours)
python train.py --map australia_100x100 --total-timesteps 1000000
```

### For Medium Maps (512√ó512, plains)

```bash
# Option A: Standard (if territory stays connected)
python train.py --map plains --total-timesteps 1000000

# Option B: Clusters (if territory splits often)
python train_clusters.py --map plains --total-timesteps 1000000
```

### For Large Maps (1024√ó1024+)

```bash
# Phase 1: Validate (200K steps, ~2 hours)
python train_multiscale.py \
  --map australia_1024x1024 \
  --total-timesteps 200000

# Phase 2: Full training (2M steps, ~20 hours)
python train_multiscale.py \
  --map australia_1024x1024 \
  --total-timesteps 2000000
```

---

## Migration Guide

### From Phase 3 to Phase 5

**Phase 3 models ARE compatible** with:
- ‚úÖ `train.py` (single-scale)
- ‚ùå `train_multiscale.py` (different obs space)
- ‚ùå `train_clusters.py` (different action space)

**To continue Phase 3 training**:
```bash
python train.py \
  --continue-from ../../phase3-implementation/runs/run_XXXXX/model_final \
  --map plains \
  --total-timesteps 2000000
```

### From Single-Scale to Multi-Scale

**Not directly compatible** - must train from scratch.

Different observation spaces prevent model transfer.

### From Standard to Clusters

**Not directly compatible** - must train from scratch.

Different action spaces (Discrete vs MultiDiscrete).

---

## What's Next?

### Already Implemented ‚úÖ
- Single-scale training
- Multi-scale with attention
- Cluster-aware with action masking
- GPU support (CUDA/MPS)
- Action masking (MaskablePPO)

### Not Yet Implemented ‚è≥
- Multi-scale + clusters combined
- Self-play training
- Boat/transport mechanics
- Hierarchical RL
- Curriculum learning

### Future Enhancements üîÆ
- Meta-learning across maps
- Transfer learning
- Population-based training
- Multi-agent coordination

---

## Troubleshooting

### "Which option should I use?"

**Start with**: `train.py` (simplest)

**Upgrade to clusters if**: Territory splits are hurting performance

**Upgrade to multi-scale if**: Map is ‚â•1024√ó1024

### "Training is too slow"

1. Reduce `--n-envs`
2. Reduce `--batch-size`
3. Use simpler model (single-scale instead of multi-scale)
4. Use smaller map

### "Out of memory"

1. Reduce `--n-envs` (fewer parallel environments)
2. Reduce `--batch-size`
3. Use single-scale instead of multi-scale
4. Close other applications

### "Agent not learning"

1. Train longer (5M+ steps for complex scenarios)
2. Check TensorBoard for reward trends
3. Verify map is loading correctly
4. Try different learning rate (`--learning-rate 3e-4`)

---

**Choose your training option and start building your strategy AI!** üöÄ

See individual documentation for details:
- `README_PHASE5.md` - Complete Phase 5 guide
- `MULTISCALE_TRAINING.md` - Multi-scale details
- `CLUSTER_AWARENESS.md` - Cluster system details
- `ATTENTION_ARCHITECTURE.md` - Attention mechanisms
- `QUICK_START.md` - Getting started quickly
